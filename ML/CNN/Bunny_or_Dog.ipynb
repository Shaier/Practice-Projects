{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bunny or Dog.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "_tX9WjlRR7Ut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Mount my drive- run the code, go to the link, accept.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zbQIr20PR98x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43f5721d-1b54-4f3f-8d69-b3da621e30a2"
      },
      "cell_type": "code",
      "source": [
        "#Change working directory to make it easier to access the files- (Folder inside of Colab- CNN folder- Images- Train/Test folder- Inside of each there are bunny/dog folders) \n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/Simple CNN Image Tutorial\")\n",
        "os.getcwd() "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/Simple CNN Image Tutorial'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "D4ng-z46SyrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80f95141-706b-458b-cf81-c59acb28e64a"
      },
      "cell_type": "code",
      "source": [
        "#Check if the images were uploaded\n",
        "img_folder = 'images'\n",
        "#1. Get sample file\n",
        "#2. Read image and display\n",
        "from IPython.display import Image\n",
        "Image(\"Cute-puppy-photos-82.jpg\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "Cute-puppy-photos-82.jpg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "ufrXLGhpVhRc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import h5py\n",
        "import csv\n",
        "\n",
        "from scipy.misc import imresize, imsave\n",
        "\n",
        "from sklearn.metrics import log_loss, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from PIL import Image, ImageChops, ImageOps\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dropout, Flatten, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2TMgQdkXjmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "de92276e-77b1-434a-9b83-d7d152243a3d"
      },
      "cell_type": "code",
      "source": [
        "#Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "#Step 1- Convolution\n",
        "#Make 32 feature detectors (filters/kernels) with a size of 3x3\n",
        "#Choose the input-image's format to be 64x64 with 3 channels\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation=\"relu\"))\n",
        "\n",
        "# Step 2 - Pooling - the window is 2x2 pixels\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "#Make 32 feature detectors (filters/kernels) with a size of 3x3\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2))) #the window is 2x2 pixels\n",
        "\n",
        "# Step 3 - Flattening- transforming the NxN matrix to Nx1 (A.K.A 1 column)\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection - \"units\" is the dimensionality of the output space - so here we send it to 128 neurons and then all of those go to 1 neuron\n",
        "classifier.add(Dense(activation=\"relu\", units=128))\n",
        "classifier.add(Dense(activation=\"sigmoid\", units=1))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# use ImageDataGenerator to preprocess the data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Augment the data so we can \"create\" a larger dataset \n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "#Prepare the training data:\n",
        "#1st input is the directory where the different folders of images are (folder of bunnies, folder of dogs here)\n",
        "#2nd input is target_size=  tuple of integers (height, width) - default: `(256, 256)`. The dimensions to which all images found will be resized.\n",
        "#3rd input is batch= size of the batches of data (default: 32).One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\". Determines the type of label arrays that are returned:\n",
        "#\"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels, \"input\" will be images identical to input images (mainly used to work with autoencoders).\n",
        "#If None, no labels are returned (the generator will only yield batches of image data, which is useful to use with model.predict_generator(),  model.evaluate_generator(), etc.). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of directory for it to work correctly.\n",
        "#Note that you can also subset it here with \"subset\"- to create validation / training \n",
        "#batch_size determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around.\n",
        "training_data = train_datagen.flow_from_directory('./images/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "#Prepare the test data\n",
        "test_data = test_datagen.flow_from_directory('./images/test',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "#Start the computation\n",
        "#Generator- a python training data batch generator; endlessly looping over its training data\n",
        "#steps_per_epoch the number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size.\n",
        "#validation_steps similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter.\n",
        "#To improve the model accuracy you can increase the number of steps_per_epoch to e.g. 8000\n",
        "#The number of samples processed for each epoch is batch_size * steps_per_epochs (here the batch size is 32, so the samples processed will be 8000 per epoch)\n",
        "\n",
        "classifier.fit_generator(training_data,\n",
        "                         steps_per_epoch = (8000 / 32),\n",
        "                         epochs = 2,\n",
        "                         validation_data = test_data,\n",
        "                         validation_steps = 8000/32)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 152 images belonging to 2 classes.\n",
            "Found 23 images belonging to 2 classes.\n",
            "Epoch 1/2\n",
            "250/250 [==============================] - 152s 607ms/step - loss: 0.3724 - acc: 0.8274 - val_loss: 1.6392 - val_acc: 0.5652\n",
            "Epoch 2/2\n",
            "250/250 [==============================] - 150s 599ms/step - loss: 0.0477 - acc: 0.9863 - val_loss: 2.6794 - val_acc: 0.5217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5bcecb828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "3MSGz_ltZcUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "31998d33-75e3-4b2b-8082-4ce2e0bf2918"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "#Show a new image - an image that you want to test out on the model\n",
        "from IPython.display import Image\n",
        "Image(\"./newimages/bunny3.jpg\")\n",
        "\n",
        "#To make predictions on a the new image\n",
        "#target_size ‘squishes’ the photos down to appropriate size.\n",
        "#image.img_to_array converts a PIL (Python Imaging Library) image instance to a Numpy array.\n",
        "#np.expand_dims(a,axis) expands the shape of an array. Insert a new axis that will appear at the axis position in the expanded array shape. \n",
        "#classifier.predict(test_image) returns an array of integers\n",
        "#predict_classes (docs) outputs A numpy array of class predictions. Which in your model case, the index of neuron of highest activation from your last(softmax) layer. [[0]] means that your model predicted that your test data is class 0. (usually you will be passing multiple image, and the result will look like [[0], [1], [1], [0]])\n",
        "#e.g You must convert your actual label (e.g. 'cancer', 'not cancer') into binary encoding (0 for 'cancer', 1 for 'not cancer') for binary classification. Then you will interpret your sequence output of [[0]] as having class label 'cancer'\n",
        "\n",
        "test_image = image.load_img('./newimages/bunny1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = classifier.predict(test_image)\n",
        "\n",
        "#Training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'puppy'\n",
        "else:\n",
        "    prediction = 'bunny'\n",
        "    \n",
        "print(result)\n",
        "print(prediction)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]]\n",
            "bunny\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u0u-0glOd9kh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving / uploading the weights for the model\n",
        "\n",
        "#Model weights are saved to HDF5 format. This is a grid format that is ideal for storing multi-dimensional arrays of numbers.\n",
        "#The model structure can be described and saved using two different formats: JSON and YAML\n",
        "\n",
        "#JSON\n",
        "#Save model to JSON\n",
        "model_json = classifier.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "#Serialize weights to HDF5\n",
        "classifier.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "#Load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "#Load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "#Evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
        "\n",
        "\n",
        "#ORRRRRR\n",
        "\n",
        "# serialize model to YAML\n",
        "model_yaml = classifier.to_yaml()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "classifier.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "# load YAML and create model\n",
        "yaml_file = open('model.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
